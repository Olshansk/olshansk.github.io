<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Data Science for Software Engineers — Joint Probability Matrices</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Data Science for Software Engineers — Joint Probability Matrices</h1>
</header>
<section data-field="subtitle" class="p-summary">
Joint Probability Matrices — An “Extension of the Confusion Matrix” for Continuous Variables
</section>
<section data-field="body" class="e-content">
<section name="a19e" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="6bb9" id="6bb9" class="graf graf--h3 graf--leading graf--title">Data Science for Software Engineers — Joint Probability Matrices</h3><h3 name="1dfc" id="1dfc" class="graf graf--h3 graf-after--h3">Joint Probability Matrices — An “Extension of the Confusion Matrix” for Continuous Variables</h3><p name="fea7" id="fea7" class="graf graf--p graf-after--h3 graf--trailing"><em class="markup--em markup--p-em">If you’re just interested in the code, the Jupyter notebook is available </em><a href="https://github.com/Olshansk/data_science_for_swe/blob/master/transition_matrix/Transition%20Matrix.ipynb" data-href="https://github.com/Olshansk/data_science_for_swe/blob/master/transition_matrix/Transition%20Matrix.ipynb" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">here</em></a><em class="markup--em markup--p-em">. Big thanks to </em><a href="https://www.linkedin.com/in/altarovici/" data-href="https://www.linkedin.com/in/altarovici/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">Albert Altarovici</em></a><em class="markup--em markup--p-em"> for explaining fundamental Data Science concepts and reviewing this article.</em></p></div></div></section><section name="5993" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="2533" id="2533" class="graf graf--p graf--leading">When measuring the performance of a prediction model (i.e. a Machine Learning classification algorithm), there are 4 metrics you are likely to measure and reference: <a href="https://en.wikipedia.org/wiki/Precision_and_recall" data-href="https://en.wikipedia.org/wiki/Precision_and_recall" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Precision, Recall</a>, Accuracy and <a href="https://en.wikipedia.org/wiki/F1_score" data-href="https://en.wikipedia.org/wiki/F1_score" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">F-score.</a> A common visual method to interpret these results is via a <a href="https://en.wikipedia.org/wiki/Confusion_matrix" data-href="https://en.wikipedia.org/wiki/Confusion_matrix" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Confusion Matrix</a>. A lot of great articles have already been <a href="https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62" data-href="https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">written</a> about these topics, so I won’t delve into too many details.</p><p name="ec50" id="ec50" class="graf graf--p graf-after--p">For a binary classification problem, the ground truth data (also referred to as target values) are binary (0 or 1), and the predicted value is some value between 0 and 1. There are a few things to keep in mind here:</p><ul class="postList"><li name="5de2" id="5de2" class="graf graf--li graf-after--p">A threshold needs to be selected to determine if a predicted value should be rounded up or down. This threshold needs to be chosen based on some cutoff that balances tradeoffs in your model.</li><li name="8af8" id="8af8" class="graf graf--li graf-after--li">For categorical data (more than two options), we need to use <a href="https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/" data-href="https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">one-hot encoding</a> and perform multiple pairwise analyses.</li><li name="5892" id="5892" class="graf graf--li graf-after--li">Classification metrics (accuracy, prediction, recall, f1-score) are only defined for discrete values. When predicting a continuous value, we are generally more interested in regression metrics (e.g. RMSE, MAE) as described well in this <a href="https://stackoverflow.com/questions/49103139/calculating-accuracy-scores-of-predicted-continuous-values" data-href="https://stackoverflow.com/questions/49103139/calculating-accuracy-scores-of-predicted-continuous-values" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Stack Overflow answer</a>.</li></ul><p name="f813" id="f813" class="graf graf--p graf-after--li graf--trailing">I was recently introduced to joint probability matrices, which can be thought of as an extension to confusion matrices when your data is non-categorical, but you still want to visualize the data patterns in a similar fashion. Most importantly, it can be used to easily identify outliers in your data, which will lead you to specific examples where a large discrepancy occurs between your target values and predicted values, which will hopefully help you fine tune your model. This is easiest to illustrate via an example.</p></div></div></section><section name="5242" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="2e28" id="2e28" class="graf graf--p graf--leading">Consider a class of 30 students who took a test and received a grade between 0 and 100 that is normally distributed. Let’s assume that the mean and standard deviation are 80% and 20% respectively; this will be our ground truth data. Our model will predict a grade for each student that is also between 0 and 100. For simplicity, we’ll model our predictions using a random set of numbers that has the same distribution as the actual data.</p><p name="a7a3" id="a7a3" class="graf graf--p graf-after--p">The following snippet of code will generate our mocked data:</p><figure name="a9bc" id="a9bc" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/8377249322503c131788c57940822b24.js"></script></figure><p name="b4c1" id="b4c1" class="graf graf--p graf-after--figure">Next, we illustrate a few methods of comparing our ground truth data to our predicted data using regression analysis methods. The following snippet of code will output several regression metric values, plot the <a href="https://en.wikipedia.org/wiki/Probability_density_function" data-href="https://en.wikipedia.org/wiki/Probability_density_function" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Probability Density Function</a>, as well as a histogram showing the frequency distribution of the grades. <a href="https://heartbeat.fritz.ai/" data-href="https://heartbeat.fritz.ai/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Fritz AI</a> has a <a href="https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0" data-href="https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">fantastic article</a> about the different regression analysis methods and their tradeoffs, so we won’t delve into the details.</p><figure name="d8d9" id="d8d9" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/b6b5e49512bc642fdbe95900959024c3.js"></script></figure><p name="cf1a" id="cf1a" class="graf graf--p graf-after--figure">The code snippet above will generate the following output:</p><pre name="0a99" id="0a99" class="graf graf--pre graf-after--p">mean_squared_error:  733.4<br>mean_absolute_error:  21.54<br>explained_variance_score:  -1.15</pre><p name="6120" id="6120" class="graf graf--p graf-after--pre">And the following graph:</p><figure name="7b81" id="7b81" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*O_W3XR3dDvAT73YwpdvehQ.png" data-width="1016" data-height="957" src="https://cdn-images-1.medium.com/max/800/1*O_W3XR3dDvAT73YwpdvehQ.png"></figure><p name="8e79" id="8e79" class="graf graf--p graf-after--figure">From the graphs above, we see that the two data sets follow a similar pattern, which is expected because they’re both normally distributed with the same mean and standard deviation. However, the regression analysis metrics (i.e. Mean Squared Error) are showing that our data is essentially senseless, which is also expected because we generated our data randomly. This means that while our on aggregate, the distribution is similar, individual values (e.g. predicted grade of Student A vs actual grade of StudentA) are very different.</p><p name="b870" id="b870" class="graf graf--p graf-after--p graf--trailing">In order to determine where the biggest variations lie, we can use a joint probability matrix.</p></div></div></section><section name="398b" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="10cb" id="10cb" class="graf graf--p graf--leading">The joint probability matrix for the data above will look as follows:</p><figure name="8e46" id="8e46" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1JPRowHYciToCXH36VqdaQ.png" data-width="462" data-height="377" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*1JPRowHYciToCXH36VqdaQ.png"></figure><p name="3c7f" id="3c7f" class="graf graf--p graf-after--figure">How do we interpret this data? For example, the red title on the 2nd last row, with a value of 0.07 means that 7% of the students (2 in our case) who actually received a grade between 80%-90%, were predicted to receive a grade between 50%-60% by our model. Next, one would manually inspect this small portion of hand-picked outliers and start drawing conclusions about changes that need to be made to the prediction model. This sort of analysis could lead to interesting followup investigations, but was obfuscated by the regression analysis done above. Perhaps the model is is very biased to under-predict the grade of high performers, perhaps it is just a bug in the code, etc…</p><p name="8a82" id="8a82" class="graf graf--p graf-after--p">The table above can be generated using the following code snippet:</p><figure name="bba1" id="bba1" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/3f2acd5715725029679d0d9d0430900e.js"></script></figure><p name="ef98" id="ef98" class="graf graf--p graf-after--figure">By making the following function call:</p><pre name="7544" id="7544" class="graf graf--pre graf-after--p">bins = np.linspace(0, 100, 11)<br>create_joint_probability_matrix(grades_GT, grades_P, bins)</pre><p name="8965" id="8965" class="graf graf--p graf-after--pre">All you need to provide is a list of the target values, the predicted values, and a list of bins of how you want to discretize the data.</p><p name="c283" id="c283" class="graf graf--p graf-after--p">In the ideal scenario, where our model is 100% accurate in predicting our ground truth data, all of the values in the joint probability matrix will lie across the diagonal. For example, building a joint probability between the the same set of values will produce the following output:</p><pre name="aabd" id="aabd" class="graf graf--pre graf-after--p">create_joint_probability_matrix(grades_GT, grades_GT, bins)</pre><figure name="1c5b" id="1c5b" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*rv5AU7L9NM4fcaO_DB_vhQ.png" data-width="468" data-height="375" src="https://cdn-images-1.medium.com/max/800/1*rv5AU7L9NM4fcaO_DB_vhQ.png"></figure><p name="c6ff" id="c6ff" class="graf graf--p graf-after--figure">For example, the table above shows that 27% of students who had an actual grade between 60–70%, were also predicted to have a grade between 60–70%. More importantly, all cells outside of the diagonal above are 0, meaning our model made no mistakes (because we’re comparing the exact same set of values).</p><p name="b784" id="b784" class="graf graf--p graf-after--p">That’s all there is to it. This doesn’t replace anything in your toolbox, but simply provides an additional method to inspect discrepancies in your predicted data for continuous variables, which can hopefully lead you down the right path in tuning your model.</p><p name="818d" id="818d" class="graf graf--p graf-after--p graf--trailing"><em class="markup--em markup--p-em">Feel free to stop reading now if you’re not interested in a breakdown of the create_joint_probability_matrix function above.</em></p></div></div></section><section name="5db3" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="9a58" id="9a58" class="graf graf--p graf--leading">For those who are less familiar with Pandas, like myself, this section will describe some of the operations we performed above.</p><p name="94c5" id="94c5" class="graf graf--p graf-after--p">Once we have our data, we need to split it into 10 equally sized buckets. This decision is kind of arbitrary and up to the discretion of the analyst.</p><figure name="1099" id="1099" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/b7d1c569f5a436b453438de88663cae5.js"></script></figure><p name="5835" id="5835" class="graf graf--p graf-after--figure">The output of the the cut function will simply assign a range (i.e. a bucket) to each value. The output of the above code snippet will output:</p><figure name="a92a" id="a92a" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*nxWf05lxYsnHXJch19JGVw.png" data-width="173" data-height="184" src="https://cdn-images-1.medium.com/max/800/1*nxWf05lxYsnHXJch19JGVw.png"></figure><p name="ea08" id="ea08" class="graf graf--p graf-after--figure">Next, we need to transform the DataFrames so they can be merged together:</p><figure name="7037" id="7037" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/e32d35eba69a270e8966ef37a8ab9866.js"></script></figure><p name="5fce" id="5fce" class="graf graf--p graf-after--figure">I personally found it easiest to understand what the commands above do by inspecting small portions of the DataFrame at each step.</p><ol class="postList"><li name="2925" id="2925" class="graf graf--li graf-after--p">Assign a student ID (i.e. index) to each student and associate it with the bucket that their grade is in; both in the predicted and ground truth tables.</li><li name="9f38" id="9f38" class="graf graf--li graf-after--li">Merge the two tables based on the student ID.</li><li name="3895" id="3895" class="graf graf--li graf-after--li">Create a multi-leveled pandas DataFrame that provides a count of the number of students</li></ol><figure name="dd53" id="dd53" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*Gr4WqRcm1mt6rv1VDpJJEw.png" data-width="317" data-height="769" src="https://cdn-images-1.medium.com/max/800/1*Gr4WqRcm1mt6rv1VDpJJEw.png"></figure><p name="8ddd" id="8ddd" class="graf graf--p graf-after--figure">Next, we take the 2D array and convert the count values to percentages based on the total number of students in our dataset:</p><figure name="b4a0" id="b4a0" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/3987754546ad715fe32015c9078aa26f.js"></script></figure><p name="bee1" id="bee1" class="graf graf--p graf-after--figure">This will produce the following output:</p><figure name="4ea7" id="4ea7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BtQ7bTImSg8vp2PJUf7Gmg.png" data-width="543" data-height="318" src="https://cdn-images-1.medium.com/max/800/1*BtQ7bTImSg8vp2PJUf7Gmg.png"></figure><p name="6f06" id="6f06" class="graf graf--p graf-after--figure">A quick check to make sure this step is correct is by verifying that the sum of all values in your DataFrame add up to 1.</p><p name="2ea2" id="2ea2" class="graf graf--p graf-after--p">Lastly, we simply want to format the output table so it’s more readable by rounding our values, and applying a background gradient to easily see where the outlier cells are. The following code snippet:</p><figure name="e39f" id="e39f" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/9af5b101d7a27e1f17a75684458bd9dc.js"></script></figure><p name="1336" id="1336" class="graf graf--p graf-after--figure">Will format the table above into this one:</p><figure name="f664" id="f664" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*iOSRMdGfWEoTVqHvv2c0IA.png" data-width="462" data-height="377" src="https://cdn-images-1.medium.com/max/800/1*iOSRMdGfWEoTVqHvv2c0IA.png"></figure></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@olshansky" class="p-author h-card">Daniel Olshansky</a> on <a href="https://medium.com/p/f4fcb94d1483"><time class="dt-published" datetime="2020-05-25T23:31:48.536Z">May 25, 2020</time></a>.</p><p><a href="https://medium.com/@olshansky/data-science-for-software-engineers-joint-probability-matrices-f4fcb94d1483" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on September 13, 2025.</p></footer></article></body></html>