<p>#TIL I caught up on a handful of cool things in the world LLM enabling infrastructure.</p><p>https://abishekmuthian.com/how-i-run-llms-locally/?utm_source=tldrnewsletter</p><h2><strong>What do I pay for?</strong></h2><ul><li><p>Only refers to LLM Developer tooling, not products</p></li><li><p>ChatGPT Pro</p></li><li><p>Claude Pro</p></li><li><p>Before you bring it up, I don’t pay for things like Supermaven, cursor, etc…</p></li></ul><h2><strong>Cursor</strong></h2><ul><li><p>Building the muscle to help with refactor</p></li><li><p>The moment I feel my brain stop working, I try to see if AI can do it</p></li><li><p>It’s akin to renaming things in the past </p></li></ul><h2><strong>Cursor / RAG Pipeline</strong></h2><p></p><p></p><h2><strong>Ollama</strong></h2><ul><li><p><strong>Website</strong>: Their <a href="https://ollama.com/search">models directory</a></p></li><li><p><strong>Interface</strong>: a</p></li></ul><h2><strong>Cerebras</strong></h2><ol><li><p><strong>Supabase</strong></p></li></ol><ol><li><p><strong>HugingFace</strong></p></li></ol><ol><li><p><strong>OpenRouter</strong></p></li></ol><ol><li><p>Grok/x</p></li></ol><h2><strong>AI Gateways</strong></h2><ul><li><p><a href="https://konghq.com/products/kong-ai-gateway">https://konghq.com/products/kong-ai-gateway</a></p></li><li><p><a href="https://portkey.ai/features/ai-gateway">https://portkey.ai/features/ai-gateway</a></p></li><li></li></ul><ol><li><p><strong>Ollama</strong> is still the easiest way to run models locally. It’s easy. Their UI is on point. It’s hard to believe how great of a product this is. [1]</p></li><li><p><strong>Cerebras</strong> inference really is ULTRA fast. I don’t need anything this fast, but I appreciate someone is building it.</p><ol><li><p>They’re public about how they do this using CS-3. If you’re in software (like me), this is what real engineering looks like [2]</p></li><li><p>Their main README is just one page with all the APIs you’ll need. Best of all, they’re all OpenAI compatible. [3]</p></li><li><p>They have the best (short &amp; to the point) example of RAG using docker + pinecone (my go-to stack). [4]</p></li></ol></li><li><p><strong>Supabase</strong> released <strong><a href="http://database.build/">database.build</a></strong> which lets you explain the idea you need and they build the schema for you. Then you can deploy the database with the click of a button. Amazing!!! [5]</p><ol><li><p>The full blog post is full of juicy details [6]</p></li><li><p>PGLite enables running a full Postgres database locally (in the browser) using WASM that is both reactive and syncs live [7]</p></li><li><p>Electric is like GraphQL but for Postgres. It has an HTTP API that lets you query Postgres “shapes”.</p></li></ol></li><li><p><strong>Hugginface</strong> has direct integration with ollama. It doesn’t work for every model out of the box, but for some, you can just run one command and it’s available locally:</p></li></ol><p>```</p><p>ollama run <a href="http://hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:IQ4_XS">hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF:IQ4_XS</a></p><p>```</p><p>https://huggingface.co/docs/hub/en/ollama</p><ol><li><p><strong>Postgres</strong> has a lot of AI related extensions</p></li></ol><p>[6] <a href="https://github.com/Olshansk/postgres_for_everything">https://github.com/Olshansk/postgres_for_everything</a></p><ol><li><p><strong>OpenRouter.AI</strong> has two login options: Google &amp; MetaMask. Interesting…</p></li></ol><ol><li></li></ol><h2><strong>Uncensored Models</strong></h2><ul><li><p>https://huggingface.co/Guilherme34/Llama-3.2-11b-vision-uncensored/discussions/3</p></li></ul><ul><li><p>The benefit of open source is to have access to uncensored models</p></li><li><p>The truth is:</p><ul><li><p>We use whatever is off the shelf</p></li><li><p>They’re harder to use</p></li><li><p>They’re harder to find</p></li><li><p></p></li></ul></li><li><p></p></li></ul><p></p><h2><strong>Pydantic AI</strong></h2><ul><li><p><strong>Good idea</strong>: https://www.anthropic.com/news/model-context-protocol</p></li><li><p><strong>Good execution</strong>: https://ai.pydantic.dev/models/#environment-variable</p></li></ul><p>https://ollama.com/</p><p>[2] <a href="https://cerebras.ai/product-system/">https://cerebras.ai/product-system/</a></p><p>[3] <a href="https://github.com/Cerebras/cerebras-cloud-sdk-python">https://github.com/Cerebras/cerebras-cloud-sdk-python</a></p><p>[4] <a href="https://github.com/Cerebras/inference-examples/tree/main/rag-pinecone-docker">https://github.com/Cerebras/inference-examples/tree/main/rag-pinecone-docker</a></p><p>[5] </p><p>https://database.build/</p><p>[6] <a href="https://supabase.com/blog/database-build-v2">https://supabase.com/blog/database-build-v2</a></p><p>[7] https://pglite.dev/</p><p>[8] </p><p>https://electric-sql.com/</p><p>[5] <a href="https://huggingface.co/docs/hub/en/ollama">https://huggingface.co/docs/hub/en/ollama</a></p><p>[7] <a href="https://platform.openai.com/docs/examples">https://platform.openai.com/docs/examples</a></p><p>[9] </p><p>https://openrouter.ai/</p><p>Publish all of the above in notes: https://olshansky.substack.com/notes</p>