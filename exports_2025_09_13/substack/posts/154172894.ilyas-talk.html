<p>Talk of the town is ilya</p><ol><li><p>Data</p></li><li><p>Moment where he also cries</p></li><li><p>We can consume all of Reddit, YouTube, News, tc…</p></li><li><p>We consume all of Slack, SalesForce, Bloomberg, Discord, teams</p></li><li><p>Data gets repetitive</p></li><li><p>RLHF -&gt; higher quality</p></li><li><p>Look at the new phi model</p></li><li><p>Link to my blog</p></li><li><p>Mention the phi models</p></li><li></li></ol><p><a href="https://olshansky.substack.com/p/an-incentive-to-label">https://olshansky.substack.com/p/an-incentive-to-label</a></p><p><a href="https://simonwillison.net/2024/Dec/15/phi-4-technical-report/#atom-everything">https://simonwillison.net/2024/Dec/15/phi-4-technical-report/#atom-everything</a></p><div id="youtube2-1yvBqasHLZs" class="youtube-wrap" data-attrs="{&quot;videoId&quot;:&quot;1yvBqasHLZs&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><div class="youtube-inner"><iframe src="https://www.youtube-nocookie.com/embed/1yvBqasHLZs?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></div></div><div id="youtube2-1yvBqasHLZs" class="youtube-wrap" data-attrs="{&quot;videoId&quot;:&quot;1yvBqasHLZs&quot;,&quot;startTime&quot;:&quot;780&quot;,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><div class="youtube-inner"><iframe src="https://www.youtube-nocookie.com/embed/1yvBqasHLZs?start=780&amp;rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></div></div><p>“Still to see it, it’s just unbelievable. I can’t convey that feeling to you.”</p><p><a href="https://arxiv.org/abs/2412.08905">https://arxiv.org/abs/2412.08905</a></p><p>We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. U</p><p>The new phi-4 model is another classic example of data quality &gt;&gt;&gt; data quantity.</p><p>We needed "big data" to get a foundation model, but need "quality data" to get to AGI.</p><p>A "foundation model" using "big data" is akin from a child growing from the age of 0 to 10 building a foundational model of the world.</p><p>However, intelligence that drives changes is usually founded on the formative years of 10 to 20 where the quality of your environment and surroundings are a lot more critical.</p><p>We present phi-4 [...] focused on data quality. Unlike most language models [...] strategically incorporates synthetic data throughout the training process. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size [...] due to improved data [...]</p><p><a href="https://simonwillison.net/2024/Dec/15/phi-4-technical-report/#atom-everything">https://simonwillison.net/2024/Dec/15/phi-4-technical-report/#atom-everything</a></p><p><a href="https://arxiv.org/abs/2412.08905">https://arxiv.org/abs/2412.08905</a></p><p>I've been thinking of rewriting this whole blog post as my thinking is still mostly the same, but how I want to communicate this problem and opportunity has fundamentally changed.</p>