+++
author = "Daniel Olshansky"
title = "Why was Sam Altman fired from OpenAI?"
date = "2023-11-18T20:32:16.134Z"
description = "tl;dr AGI (Artificial General Intelligence) may be closer than we think."
tags = [
    "ai", "startup", "tech", "productivity", "book"
]
substack_url = "https://olshansky.substack.com/p/why-was-sam-altman-fired-from-openai"
+++

If you’ve visited [X](https://twitter.com/search?q=openai) in the last 24 hours, seen [OpenAI’s Blog](https://openai.com/blog/openai-announces-leadership-transition), or simply opened up Google, you’ve probably heard that **the board of OpenAI has fired its CEO, Sam Altman**. There are already dozens of articles online covering the topic, including [Ars Technica](https://arstechnica.com/information-technology/2023/11/report-sutskever-led-board-coup-at-openai-that-ousted-altman-over-ai-safety-concerns/), [The Information](https://www.theinformation.com/articles/before-openai-ousted-altman-employees-disagreed-over-ai-safety), [Wired](https://www.wired.com/story/openai-sam-altman-ousted-what-happened/), [The Verge](https://www.theverge.com/23966325/openai-sam-altman-fired-turmoil-chatgpt), [CNN](https://www.cnn.com/2023/11/17/tech/sam-altman-departs-open-ai/index.html) and many others.

I got a flurry of messages yesterday afternoon simply letting me know of this fact (my favourite one below). After a day of more information coming in, I figured I’d share my thoughts because I believe it may be a signal that AGI is closer than we think. I’m simultaneously excited and don’t know what to expect.

Note that I did not end up reaching 1 billion people, but it remains a life goal through other avenues.I’ve been following Sam [since he was first appointed as President of YC](https://www.ycombinator.com/blog/sam-altman-for-president/) in 2014, and he is someone who both inspires me and has taught me a lot (from afar) over the years, so I was shocked to hear the news just like many others.

- [https://twitter.com/sama/status/1725742088317534446](https://twitter.com/sama/status/1725742088317534446)I won’t reiterate exactly what went down because Greg Brockman’s ([@gdb](https://twitter.com/gdb)) tweet captures the gist.

[https://twitter.com/gdb/status/1725736242137182594](https://twitter.com/gdb/status/1725736242137182594)[OpenAI’s Announcement of the Leadership Transition](https://openai.com/blog/openai-announces-leadership-transition) mentioned that this was a decision of the board, consisting of:

**Ilya Sutskever** - OpenAI Chief Scientist & **Co-Founder**

- Adam D’Angelo - Quora CEO 

- Tasha McCauley - Technology entrepreneur 

- Helen Toner - Georgetown Center for Security and Emerging Technology’s 

- **[Stepped Down] Greg Brockman** - OpenAI President & **Co-Founder**

I decided to cross-reference this list with the list of co-founders ([according to Wikipedia](https://en.wikipedia.org/wiki/OpenAI)):

> The organization was founded in December 2015 by **Ilya Sutskever**, **Greg Brockman**, **Trevor Blackwell**, **Vicki Cheung**, **Andrej Karpathy**, **Durk Kingma**, **Jessica Livingston**, **John Schulman**, **Pamela Vagata**, and **Wojciech Zaremba**, with **Sam Altman** and **Elon Musk** serving as the initial board members

Aside from the fact that there are almost a dozen co-founders, it was interesting that with the exception of Greg, who stepped down, Ilya is the only other individual on both lists.

For those that don’t know, [Ilya Sustkever](https://www.cs.toronto.edu/~ilya/) studied under [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton), the “Godfather of Deep Learning,” at the University of Toronto and did a postdoc under [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng), “The Godfather of AI Education,” at Stanford. **In my opinion, if we had to look to a single individual leading the AI Revolution, it’s Ilya.**

In [Walter Isaacson recent biography of Elon Musk](https://www.goodreads.com/en/book/show/122765395), he mentions how much effort Elon put into persuading Ilya to move from Google to OpenAI and lead the charge on AGI safety. He was the reason Elon & Larry Page stopped being friends, and Elon talks about it again in a recent podcast he did with Lex Friedman (timestamped below).

The point I’m getting to is that AI safety is at the top of Ilya’s mind, and has been since day one. It’s the reason he introduced the [Superalignment effort](https://openai.com/blog/introducing-superalignment) last July, dedicating 20% of their compute to solving the alignment problem within four years. He discusses all the challenges related to this in an interview from [last July](https://www.lesswrong.com/posts/TpKktHS8GszgmMw4B/ilya-sutskever-s-thoughts-on-ai-safety-july-2023-a) as well.

According to [The Information](https://www.theinformation.com/articles/before-openai-ousted-altman-employees-disagreed-over-ai-safety), one of the statements he made to the team after the news were made public is:

> This was the board doing its duty to the mission of the nonprofit, which is to make sure that OpenAI builds AGI that benefits all of humanity.

*Ilya Sutskever*

Wojceich, one of the other co-founders, publically reinforced that the mission of building a safe AGI to benefit humanity has not changed, but is also sad to see both Sam and Greg go.

[https://twitter.com/woj_zaremba/status/1725718242466099308](https://twitter.com/woj_zaremba/status/1725718242466099308)Having worked within teams driven by AI and ML through the 2010s, I was an AGI skeptic. I knew it would happen one day, but did not expect to see it in my lifetime. When ChatGPT came out in late 2022, I did a complete 180 and realized how I wrong I was after publishing [24 Hours of ChatGPT](https://olshansky.substack.com/p/24-hours-of-chatgpt).

Over the last week, [there have been news around GPT5](https://interestingengineering.com/culture/openai-working-on-gpt-5-ceo-altman) being underway. If the jump from GPT4 to GPT5 will resemble anything we’ve seen from GPT3 to GPT4, I genuinely don’t know what to expect. **My guess is that there was a disagreement between Sam and Ilya when finding a balance between building the business and shipping product while also maintaining AGI safety and staying true to the core of OpenAI’s vision.**

It’s hard to say what kind of innovation is going on behind the scenes at OpenAI, but they have some of the most talented people on the planet. It’s hard to say what kind of conversations they have, but the last few months are reinforcing that safety is becoming a top priority. It’s hard to say when AGI will come into existence, but I anticipate it’ll be sooner than most expect. It’s hard to say whether letting Sam go was the right decision, but only time will tell.

Subscribe for more of Olshansky's Newsletter!