---
title: "Intelligence is not Intuition"
date: 2025-10-17T10:26:53-0700
draft: false
description: "AGI is not the same as AGI"
tags: ["AI", "Philosophy", "Posts"]
categories: ["AI", "Philosophy", "Posts"]
medium_url: ""
substack_url: ""
ShowToc: true
TocOpen: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
---

Artificial General Intelligence (AGI) is here, and nothing has changed.

Artificial General Intuition (AGI) is not here yet, but it will change everything.

How do we get there?

## When will AGI be here?

If you're reading this, you've most likely listened to Andrej Karpathy's conversation with Dwarkesh about [how AGI is still a decade away](https://www.dwarkesh.com/p/andrej-karpathy).

It's one of the highest signal-to-noise conversations I've heard on the topic. But I'd argue that Artificial General Intelligence (AGI) is already here. Intuition is a decade away.

Whether you're using ChatGPT to draft an email, an online tool to generate videos, or a CLI-driven LLM agent to write code, we're using an **Artificial** tool in a **General** way that shows some degree of **Intelligence**. Sometimes I feel like people were more impressed by [Alex the parrot](<https://en.wikipedia.org/wiki/Alex_(parrot)>) than by the tools we're using today.

Billions of dollars are being poured into engineering efforts to move from intelligence to superintelligence, but the gap between superintelligence and intuition is where a new breakthrough is needed‚Äîand I don't know when (or if) it will happen.

In the meantime, this distinction is critical because **intuition** is what we need to use Artificial General Intelligence to its full capacity.

## Terminology Used in AI

If you look closely at the terminology used in and around the field of AI, you notice a gap: a lack of focus on intuition.

In [**Reinforcement Learning**](https://en.wikipedia.org/wiki/Reinforcement_learning), we use **reward functions** to **train** models to achieve specific goals. This is done through **learning** in either **supervised** or **unsupervised** ways.

When the models are **applied** in the real world, we call it **inference**, which is a fancy way of saying **prediction** based on past experience.

One could argue that the above is a form of intuition, but it still feels like something‚Äôs missing.

Intuition is not something that's taught, learned, trained, or built. It‚Äôs something that‚Äôs acquired over time.

## 10,000 hours

As I was thinking through this, I was reminded of [Malcolm Gladwell's famous 10,000-hour rule](https://pmc.ncbi.nlm.nih.gov/articles/PMC4662388/).

It‚Äôs a great way to frame the difference between raw intelligence, experience, expertise, and intuition.

With a bit of effort, most people can be trained to do most things if there‚Äôs a clear, repeatable process. It doesn‚Äôt take 10,000 hours. In fact, I remember my first job at McDonald‚Äôs as a 15-year-old. I was trained to salt french fries, and it only took an hour or so to get the hang of it üçü.

What 10,000 hours gives you isn‚Äôt just a tool‚Äîit‚Äôs a toolbox and experience. Intuition is how you translate that experience to decide which tool to use given the circumstances.

This reminds me of one of my favorite quotes by Pablo Picasso:

> ‚ÄúLearn the rules like a pro, so you can break them like an artist.‚Äù

## Intuition in AI vs. Intuition in Using AI

All of that is interesting, but how is any of it relevant today?

Understanding that these LLMs are intelligent but not intuitive is critical to defining how we use them.

The only way to leverage AI to its full capacity is by spending 10,000 hours with it.

You build intuition for what it can and cannot do. You build intuition for when to take its results verbatim and when to question them. It‚Äôs no different from hiring a brilliant new graduate fresh out of college.

Remember, AI is intelligent but not intuitive. You are.
