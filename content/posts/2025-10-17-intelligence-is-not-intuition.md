---
title: "Intelligence is not Intuition"
date: 2025-10-17T10:26:53-0700
draft: false
description: ""
tags: ["AI", "Philosophy", "Posts"]
categories: ["AI", "Philosophy", "Posts"]
medium_url: ""
substack_url: ""
ShowToc: true
TocOpen: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
---

## The Adoption Gap of AI in Other Industries

When you work at a small startup in the software space, it feels like everyone is using LLMs and agents for absolutely everything ever day.

I've been using [GitHub Copilot since the day it launched on Beta. I've been using ChatGPT since the first day it launched. I use agents to do all of my coding for me and no longer write code from scratch myself. There's no looking back.

However, when I look sideways, I am struck by the gap of adoption.

Is it a function of age? The company size? The industry? The individual's innate curiosity? The list goes on...

In reality, it's all of the above.

I've had a lot of conversations with different people, in diverse industries, with various degrees of experience, and the only common pattern I see is:

> But it goes X wrong.

## AGI is already here

Artificial General Intelligence (AGI) is already here.

All of the "agentic tools" build on top of Large Language Models (LLMs) are AGI.

It's **Artificial**. It's **General**. It's **Intelligent**.

It is not **Superintelligent**. Most importantly, it is not **Intuitive**.

## Intuition cannot be trained or learnt, it is built

If you pay close attention to all of the words used in and around the field of AI, they all start pointing to the fact that Intuition is the missing gap.

In [Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning), the

> The typical framing of a reinforcement learning (RL) scenario: an agent takes actions in an environment, which is interpreted into a reward and a state representation, which are fed back to the agent.

- Reward Functions
- Reinforcement Learning
- Training
- Supervised and unsupervised learning
- There’s a reason it’s called training and inference, not learning and applying

[Malcom Gladwell's famous 10-000 hour rule](https://pmc.ncbi.nlm.nih.gov/articles/PMC4662388/) lays the foundation for this, but it has to be called out explicitly.

The most important thing that happens during those 10-000 hours are not the skills we learn, but the intuition we build.

In the same way that that it is hard, if not impossible, to interview for this, it
is also not something we can build a benchmark for.

## The gap between

We already have Artificial General Intelligence (AGI) in the form of LLMs,
and we have Augmented Intelligence in the form of agents.

- TODO: link to Simon's post defining this.

I anticipate that we will have Super Intelligence at some point in the coming years, but I'm not sure if we'll ever have Artificial General Intuition. This is
the human touch. This is expertise. This is something that takes time to build
and I'm not even sure how to introspect on it. It really depends on my mood and
energy levels those days.

Between the two, I've recently been spending a lot of time around people who are not in the software industry and am struck by

- It’s artificial. It’s general. It’s intelligent. It’s not intuitive. It’d be fun if openai released a rubber duck that sits on your desk.
- AGI is already here
- Superintelligence is not here
- I used LLMs in a general matter
- What these models have are tools, and they can be trained to use tools and have knowledge
- I don’t know if there’s a way to build intuition, because there’s no definitive reward function
- Time spent with them
- Experience in life
- 10,000 hours
- Augmented intelligence
- There’s no artificial intuition or trifocals intelligence
- Intelligence does not equal intuition
