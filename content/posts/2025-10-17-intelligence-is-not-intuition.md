---
title: "Intelligence is not Intuition"
date: 2025-10-17T10:26:53-0700
draft: false
description: "AGI is not the same as AGI"
tags: ["AI", "Philosophy", "Posts"]
categories: ["AI", "Philosophy", "Posts"]
medium_url: ""
substack_url: ""
ShowToc: true
TocOpen: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
---

Artificial General Intelligence (AGI) is here, and nothing changed.

Artificial General Intuition (AGI) is not here yet, but will change everything.

How do we get there?

## When will AGI be here?

If you're reading this, you've most certainly listened to Andrej Karpathy's conversation with Dwarkesh about [how AGI is still a decade away](https://www.dwarkesh.com/p/andrej-karpathy).

It's one of the highest signal-to-noise conversations I've heard on the topic. But, I'd push hard that Artificial General Intelligence (AGI) is already here. Intuition is a decade away.

Whether you're using ChatGPT to draft an email, an online tool to generate videos, or a CLI driven LLM agent to write code, we're using an **Artificial** tool in a **General** way that has some degree of **Intelligence**. Sometimes I feel like people were more impressed by [Alex the parrot](<https://en.wikipedia.org/wiki/Alex_(parrot)>) than the tools we're using today.

Billions of dollars are being poured into engineering efforts to move from Intelligence to Superintelligence, but the gap between Superintelligence and Intuition is where a new breakthrough is needed, and I don't know when (if?) it will happen.

In the meantime, this distinction is critical because **Intuition** is what we need to use Artificial General Intelligence in full capacity.

## Terminology Used in AI

If you think through the terminology used in and around the field of AI, you notice how the gap in the focus on intuition.

We used **Reward Functions**

In [**Reinforcement Learning**](https://en.wikipedia.org/wiki/Reinforcement_learning), we use **Reward Functions** to **train** models
to achieve a particular goal. This is done through **learning** in either **supervised** or **unsupervised** manners.

When the models are **applied** in the real world, we call it **inference**, which is a fancy way of saying **prediction** based on past experience.

One could argue that the above is a form of intuition, but it still feels like it's missing something.

Intuition is not something that's taught, learnt, trained or built, it is something that's acquired over time.

## 10,000 hours

As I was thinking through this, [Malcom Gladwell's famous 10-000 hour rule](https://pmc.ncbi.nlm.nih.gov/articles/PMC4662388/).

It's great for framing the difference between raw intelligence, experience, expertise and intuition.

With a bit of effort, most people can be trained to do most things if there's a clear, repeatable process. It doesn't take
10-000 hours. In fact, I remember my first job at McDonald's as a 15 year old who was trained to salt some french fries
and it only took an hour or so to get the hang of it üçü.

What 10,000 hours gives you is a tool, but a toolbox and experience. Intuition is how you translate experience to decide which tool to use given the circumstances.

This reminds me of one of my favorite quotes by Pablo Picasso:

> ‚ÄúLearn the rules like a pro, so you can break them like an artist.‚Äù

## Intuition in AI vs Intuition in Using AI

All of that is cool, but how is anything I said above relevant today?

Understanding that these LLMs are intelligent but not intuitive is critical to deliniating how we use them.

The only way to leverage AI to its full capacity is by spending 10,000 hours with them.

You build intuitition of what they can or cannot do. You build intuition of when to take their result verbatim and when to question it. It's no different than hiring a brilliant new graduate out of college.

Remember, AI is intelligent but not intuitive. You are.
